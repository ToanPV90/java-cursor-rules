---
description: 
globs: 
alwaysApply: false
---
# Java Profiling Workflow / Step 4 / Compare results after refactoring

## Project Organization

The profiling setup uses a clean folder structure with everything contained in the profiler directory:

```
your-project/
└── profiler/               # ← All profiling-related files
    ├── scripts/            # ← Profiling scripts and tools
    │   └── java-profile.sh # ← Main profiling script
    ├── results/            # ← Generated profiling output
    │   ├── *.html          # ← Flamegraph files
    │   └── *.jfr           # ← JFR recording files
    ├── docs/               # ← Analysis documentation
    │   ├── profiling-comparison-analysis.md
    │   └── profiling-final-results.md
    ├── current/            # ← Symlink to current profiler version
    └── async-profiler-*/   # ← Downloaded profiler binaries
```

## Complete Comparison Workflow

### Phase 1: Pre-Refactoring Baseline (Already Done)
If you haven't captured baseline results yet:
```bash
# 1. Start your application
./mvnw spring-boot:run

# 2. Run baseline profiling
cd profiler/scripts
./java-profile.sh --mode memory --duration 60 --output-prefix baseline

# 3. Generate load to capture issues
# (Run your problematic endpoints)
```

### Phase 2: Post-Refactoring Report Generation (CRITICAL STEP)

#### Step 1: Validate Your Refactoring Changes
```bash
# Ensure your code changes are applied
git status
git diff HEAD~1  # Review recent changes
```

#### Step 2: Generate New Profiling Reports
```bash
# 1. Restart application with refactored code
./mvnw spring-boot:run

# 2. Wait for application to be fully ready
curl http://localhost:8080/actuator/health

# 3. Generate post-refactoring reports
cd profiler/scripts
./java-profile.sh --mode memory --duration 60 --output-prefix after-refactoring

# 4. Generate load identical to baseline testing
# (Run same test scenarios as baseline)

# 5. Verify reports were generated
ls -la ../results/*after-refactoring*
```

#### Step 3: Report Generation Validation
```bash
# Check that you have both sets of reports
echo "=== BASELINE REPORTS ==="
ls -la profiler/results/*baseline* || ls -la profiler/results/*before*

echo "=== POST-REFACTORING REPORTS ==="
ls -la profiler/results/*after* || ls -la profiler/results/*$(date +%Y%m%d)*

# Verify reports are not empty
wc -c profiler/results/*.html
```

### Phase 3: Performance Metrics Comparison

#### Memory Analysis Checklist
- [ ] **Memory leak detection**: Compare heap usage patterns between before/after flamegraphs
- [ ] **Allocation patterns**: Analyze allocation flamegraphs for reduced object creation  
- [ ] **GC pressure**: Compare garbage collection frequency and duration
- [ ] **Peak memory usage**: Identify improvements in maximum heap utilization
- [ ] **Stack depth**: Compare flamegraph complexity (canvas height, levels)

#### CPU Performance Checklist  
- [ ] **Hot spots identification**: Compare CPU flamegraphs to identify resolved bottlenecks
- [ ] **Method execution time**: Analyze improvements in critical path performance
- [ ] **Thread contention**: Look for reduced blocking or improved concurrency

#### Visual Comparison Process
```bash
# Open reports side by side for comparison
# Browser tab 1: Baseline report
open profiler/results/memory-leak-*baseline*.html

# Browser tab 2: After refactoring report  
open profiler/results/memory-leak-*after*.html

# Compare:
# - Canvas height (lower = better)
# - Stack depth (fewer levels = better)
# - Hot spot patterns (reduced width = better)
```

### Phase 4: Documentation Creation

#### Step 1: Create Comparison Analysis Document
```bash
# Create the comparison analysis document
touch profiler/docs/profiling-comparison-analysis.md
```

Template for `profiler/docs/profiling-comparison-analysis.md`:
```markdown
# Profiling Comparison Analysis - [Date]

## Executive Summary
- **Refactoring Objective**: [What was being fixed]
- **Overall Result**: [Success/Partial/Failed]
- **Key Improvements**: [List main improvements]

## Methodology
- **Baseline Date**: [Timestamp of baseline reports]
- **Post-Refactoring Date**: [Timestamp of after reports]
- **Test Scenarios**: [What endpoints/operations were tested]
- **Duration**: [How long profiling ran]

## Before/After Metrics
| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Stack Depth (levels) | X | Y | Z% |
| Canvas Height (px) | X | Y | Z% |
| Memory Allocations | X | Y | Z% |
| Thread Count | X | Y | Z% |

## Key Findings
### Resolved Issues
- [ ] Issue 1: [Description and evidence]
- [ ] Issue 2: [Description and evidence]

### Remaining Concerns
- [ ] Concern 1: [Description and next steps]

## Visual Evidence
- **Baseline Reports**: `../results/[filename]`
- **After Reports**: `../results/[filename]`
- **Key Differences**: [Describe what to look for in flamegraphs]

## Recommendations
1. [Next step 1]
2. [Next step 2]
```

#### Step 2: Create Final Results Summary
```bash
touch profiler/docs/profiling-final-results.md
```

### Phase 5: Troubleshooting Missing Reports

#### Problem: No New Reports Generated
```bash
# 1. Check if profiler is working
cd profiler/scripts
./java-profile.sh --help

# 2. Verify application is running
curl http://localhost:8080/actuator/health
jps | grep -i spring

# 3. Check profiler permissions
ls -la profiler/current/bin/asprof
chmod +x profiler/current/bin/asprof

# 4. Manual profiling attempt
java -jar profiler/current/lib/async-profiler.jar --help
```

#### Problem: Reports Are Empty or Corrupted
```bash
# 1. Check file sizes
ls -la profiler/results/*.html | tail -5

# 2. Verify HTML structure
head -20 profiler/results/[latest-report].html

# 3. Re-run with verbose logging
cd profiler/scripts
./java-profile.sh --mode memory --duration 30 --verbose
```

#### Problem: Can't Compare - Different Test Scenarios
```bash
# 1. Document your test scenarios
echo "Test scenarios used:" > profiler/docs/test-scenarios.md

# 2. Re-run with identical load patterns
# Use same curl commands, same timing, same duration

# 3. Automate test scenarios
cat > profiler/scripts/load-test.sh << 'EOF'
#!/bin/bash
for i in {1..10}; do
    curl http://localhost:8080/api/v1/objects/create
    curl http://localhost:8080/api/v1/threads/create
    sleep 5
done
EOF
chmod +x profiler/scripts/load-test.sh
```

## File Naming Conventions

### Recommended Naming Pattern
```bash
# Before refactoring (baseline)
memory-leak-baseline-YYYYMMDD-HHMMSS.html
allocation-flamegraph-baseline-YYYYMMDD-HHMMSS.html

# After refactoring  
memory-leak-after-refactoring-YYYYMMDD-HHMMSS.html
allocation-flamegraph-after-refactoring-YYYYMMDD-HHMMSS.html

# Alternative: timestamp-based (if baseline already exists)
memory-leak-YYYYMMDD-HHMMSS.html (earlier = baseline, later = after)
```

### File Organization Validation
```bash
# Verify you have complete sets
ls profiler/results/ | grep -E "(baseline|before)" | wc -l  # Should be > 0
ls profiler/results/ | grep -E "(after|refactoring)" | wc -l  # Should be > 0
```

## Deliverables Checklist

### Pre-Comparison Validation
- [ ] Baseline profiling results exist and are complete
- [ ] Code refactoring has been implemented and tested
- [ ] Application runs successfully with refactored code
- [ ] New profiling reports have been generated post-refactoring
- [ ] Both sets of reports use identical test scenarios

### Comparison Analysis Completed
- [ ] Side-by-side flamegraph comparison performed
- [ ] Quantitative metrics extracted and compared
- [ ] Key improvements and regressions identified
- [ ] Visual evidence documented with specific file references

### Documentation Deliverables
- [ ] `profiler/docs/profiling-comparison-analysis.md` created
- [ ] `profiler/docs/profiling-final-results.md` created  
- [ ] Quantified performance improvements documented
- [ ] Recommendations for production deployment provided
- [ ] Ongoing monitoring strategy defined

### Validation Commands
```bash
# Final validation that comparison is complete
echo "=== COMPARISON READINESS CHECK ==="
echo "Baseline reports: $(ls profiler/results/*baseline* 2>/dev/null | wc -l)"
echo "After reports: $(ls profiler/results/*after* 2>/dev/null | wc -l)"  
echo "Analysis docs: $(ls profiler/docs/profiling-*analysis* 2>/dev/null | wc -l)"
echo "Final results: $(ls profiler/docs/profiling-final* 2>/dev/null | wc -l)"
```

## Quick Start Commands

If you're missing post-refactoring reports, run these commands:

```bash
# 1. Ensure app is running with refactored code
./mvnw spring-boot:run &
sleep 30

# 2. Generate new reports
cd profiler/scripts  
./java-profile.sh --mode memory --duration 60 --output-prefix after-refactoring

# 3. Run load test while profiling
# (Execute your test scenarios)

# 4. Verify reports generated
ls -la ../results/*after*

# 5. Begin comparison analysis
echo "Ready to compare baseline vs after-refactoring reports"
```

